## 批量梯度下降

### R代码实现
```r
BatchGradientDecent <-
  function(input,output,learningRate,iter,minLoss) {
    w <- runif(n = 2,min = 0.1,max = 1)
    if (ncol(input) != length(w) || nrow(input) != length(output) || iter < 1) {
      stop()
    }
    loss <- runif(n = 1,min = 0.1,max = 1)
    vw <- list()
    for (i in 1:iter) {
      if (loss > minLoss) {
        errorSum <- output - rowSums(t(t(input) * w))
        w[1] <- w[1] + w[1] * learningRate * sum(errorSum * input[,1])
        w[2] <- w[2] + w[2] * learningRate * sum(errorSum * input[,2])
        loss <-
          0.5 * sum((output - rowSums(t(t(
            input
          ) * w))) ^ 2)
        vw[[i]] <- c(w[1],w[2],loss)
      }
    }
    return(vw)
  }
```

## 随机梯度下降

### R语言实现
```r
StochasticGradientDescent <-
  function(input,output,learningRate,iter,minLoss) {
    w <- runif(n = 2,min = 0.1,max = 1)
    n <- ncol(input)
    m <- nrow(input)
    if (n != length(w) || m != length(output) || iter < 1) {
      stop()
    }
    loss <- runif(n = 1,min = 0.1,max = 1)
    vw <- list()
    for (i in 1:iter) {
      for (j in 1:nrow(input)) {
        if (loss > minLoss) {
          errorSum <- output[j] - rowSums(t(input[j,]) * w)
          w[1] <- w[1] + w[1] * learningRate * sum(errorSum * input[j,1])
          w[2] <- w[2] + w[2] * learningRate * sum(errorSum * input[j,2])
          loss <-
            0.5 * sum((output - rowSums(t(t(
              input
            ) * w))) ^ 2)
          vw[[4*(i-1)+j]] <- c(w[1],w[2],loss)
        }
      }
    }
    return(vw)
  }
```
